package main

import (
	"context"
	"errors"
	"flag"
	"fmt"
	"io"
	"log"
	"net"
	"net/http"
	"os"
	"os/signal"
	"runtime"
	"sync"
	"sync/atomic"
	"syscall"
	"time"

	"github.com/gobwas/ws"
	"github.com/gobwas/ws/wsutil"
	"golang.org/x/sys/unix"
)

var (
	addr    = flag.String("addr", ":8080", "WebSocket service address (e.g., :8080)")
	workers = flag.Int("workers", runtime.NumCPU()*2, "Number of worker goroutines")
	// Read/Write buffer sizes for gobwas/ws helpers if needed, not directly used for epoll buffers here
	// readBufSize  = flag.Int("readBuf", 4096, "Read buffer size per connection")
	// writeBufSize = flag.Int("writeBuf", 4096, "Write buffer size per connection")
)

type epoll struct {
	fd          int
	connections sync.Map // Map[int]net.Conn

	totalSocket atomic.Int64

	workerChan  chan<- eventJob
	shutdownCtx context.Context
}

type eventJob struct {
	fd     int
	events uint32
}

func newEpoll(workerChan chan<- eventJob, shutdownCtx context.Context) (*epoll, error) {
	// EPOLL_CLOEXEC ensures the epoll FD is closed if the process execs another program
	fd, err := unix.EpollCreate1(unix.EPOLL_CLOEXEC)
	if err != nil {
		// Use log.Fatalf for critical startup errors
		log.Fatalf("Cannot create epoll instance: %v", err)
		return nil, fmt.Errorf("cannot create epoll instance: %w", err) // Added for clarity, though Fatalf exits
	}
	log.Printf("Created epoll instance with FD: %d", fd)
	return &epoll{
		fd:          fd,
		shutdownCtx: shutdownCtx,
		workerChan:  workerChan,
	}, nil
}

// Adds a connection to the epoll instance.
func (e *epoll) add(conn net.Conn) error {
	// Get file descriptor from the connection
	fd, err := getFd(conn)
	if err != nil {
		return fmt.Errorf("failed to get file descriptor for new connection: %w", err)
	}

	// Add connection FD to epoll with edge-triggered monitoring for read, write readiness, and peer close.
	// EPOLLET (Edge Triggered): Notify only on changes. Requires careful reading/writing all available data.
	// EPOLLIN: Monitor for read readiness.
	// EPOLLOUT: Monitor for write readiness (useful for handling backpressure, currently write handling is basic).
	// EPOLLRDHUP: Monitor for peer closing connection or half-closing write side. Robust way to detect closure.
	// EPOLLERR: Monitor for errors (implicitly monitored, but good to include).
	// EPOLLHUP: Monitor for hangup (implicitly monitored).
	err = unix.EpollCtl(e.fd, unix.EPOLL_CTL_ADD, fd, &unix.EpollEvent{
		Events: unix.EPOLLIN | unix.EPOLLOUT | unix.EPOLLRDHUP | unix.EPOLLET | unix.EPOLLERR | unix.EPOLLHUP,
		Fd:     int32(fd),
	})
	if err != nil {
		// If adding fails, we must close the underlying connection that was just accepted.
		log.Printf("ERROR: Failed to add FD %d to epoll: %v. Closing connection.", fd, err)
		conn.Close() // Close the connection as we can't manage it via epoll
		return fmt.Errorf("failed to add FD %d to epoll: %w", fd, err)
	}

	// Store the connection associated with the FD
	e.connections.Store(fd, conn)
	connCount := e.totalSocket.Add(1)
	log.Printf("FD %d added to epoll. Total connections: %d", fd, connCount)

	return nil
}

// Deletes a file descriptor from the epoll instance and the connection map.
func (e *epoll) delete(fd int) error {
	// Attempt to remove the FD from the kernel's epoll set
	err := unix.EpollCtl(e.fd, syscall.EPOLL_CTL_DEL, fd, nil)
	if err != nil && !errors.Is(err, unix.ENOENT) {
		// Log unexpected errors during epoll removal! This is a potential leak source.
		// ENOENT means it was already removed or never added, which is okay.
		log.Printf("!!! WARNING: EpollCtl(DEL) error for FD %d (may indicate leak if not already closed): %v", fd, err)
		// Return the error so the caller knows removal might have failed.
		return fmt.Errorf("epoll ctl del error for FD %d: %w", fd, err)
	}
	if err == nil {
		log.Printf("FD %d removed from epoll.", fd)
	} else { // ENOENT case
		log.Printf("FD %d already removed or not found in epoll (ENOENT).", fd)
	}

	// Remove from our connection map. LoadAndDelete returns the value if present.
	_, loaded := e.connections.LoadAndDelete(fd)
	if loaded {
		// Only decrement count if we actually removed it from the map
		newCount := e.totalSocket.Add(-1)
		log.Printf("FD %d removed from connection map. Remaining connections: %d", fd, newCount)
	} else {
		// This might happen if delete is called twice for the same FD due to race conditions
		// log.Printf("FD %d was already removed from connection map.", fd) // Optional log
	}

	return nil // Return nil if EpollCtl was successful or returned ENOENT
}

// Helper to get the raw file descriptor from a net.Conn
func getFd(conn net.Conn) (int, error) {
	// Check if the connection implements syscall.Conn
	sc, ok := conn.(syscall.Conn)
	if !ok {
		return -1, errors.New("connection does not implement syscall.Conn")
	}
	// Get the raw connection
	rc, err := sc.SyscallConn()
	if err != nil {
		return -1, fmt.Errorf("failed to get raw connection: %w", err)
	}

	var fd int
	var opErr error
	// Control function gets called with the raw file descriptor
	err = rc.Control(func(fdPtr uintptr) {
		fd = int(fdPtr)
	})
	if err != nil {
		return -1, fmt.Errorf("failed to control raw connection: %w", err)
	}
	// Check if the Control function itself returned an error (less common)
	if opErr != nil {
		return -1, fmt.Errorf("operation error during control: %w", opErr)
	}
	if fd <= 0 {
		return -1, errors.New("invalid file descriptor obtained")
	}
	return fd, nil
}

// Main epoll event loop. Waits for events and dispatches them to workers.
func (e *epoll) wait() {
	events := make([]unix.EpollEvent, 128) // Slightly larger buffer for events
	defer log.Println("Epoll event loop stopped.")
	log.Println("Starting epoll event loop...")

	for {
		select {
		case <-e.shutdownCtx.Done():
			log.Println("Epoll loop received shutdown signal.")
			return
		default:
		}

		// Wait for events. Timeout is -1 (infinite) to block until events or interrupt.
		// Using a timeout (e.g., 100ms) can be useful if you need to periodically check
		// the shutdown context even without events, but -1 is generally more efficient.
		nEvents, err := unix.EpollWait(e.fd, events, -1)

		if err != nil {
			// EINTR means the syscall was interrupted (e.g., by a signal), often safe to ignore and retry.
			if errors.Is(err, syscall.EINTR) {
				continue
			}
			// EBADF means the epoll FD itself was closed, likely during shutdown.
			if errors.Is(err, unix.EBADF) {
				log.Println("EpollWait error: Bad file descriptor (epoll instance closed). Exiting loop.")
				return
			}
			// Other errors might be serious.
			log.Printf("EpollWait failed: %v. Retrying shortly...", err)
			// Avoid busy-looping on persistent errors.
			time.Sleep(100 * time.Millisecond)
			continue
		}

		// Process the received events
		for i := 0; i < nEvents; i++ {
			ev := &events[i]
			job := eventJob{
				fd:     int(ev.Fd),
				events: ev.Events,
			}

			// Non-blocking send to worker channel
			select {
			case e.workerChan <- job:
				// Successfully dispatched job
			case <-e.shutdownCtx.Done():
				// Shutdown occurred while dispatching
				log.Println("Shutdown while dispatching epoll event.")
				return
			default:
				// This should ideally not happen if workers keep up and channel is buffered.
				// If it does, it might indicate workers are overwhelmed or stuck.
				log.Printf("WARNING: Worker channel full. Discarding event for FD %d (Events: 0x%x). Check worker performance.", job.fd, job.events)
			}
		}
	}
}

// Handles the WebSocket upgrade request.
func wsHander(w http.ResponseWriter, r *http.Request, ep *epoll) {
	// Upgrade HTTP connection to WebSocket
	// Note: No handshake timeout specified, might want to add one via ws.HTTPUpgrader
	conn, _, _, err := ws.UpgradeHTTP(r, w)
	if err != nil {
		// Check for specific errors if needed (e.g., EMFILE - too many open files)
		log.Printf("WebSocket upgrade error: %v", err)
		// Consider sending appropriate HTTP error (e.g., 500, or 400 for bad handshake)
		// http.Error(w, "WebSocket upgrade failed", http.StatusInternalServerError)
		return
	}

	// Add the successfully upgraded connection to epoll management
	if err := ep.add(conn); err != nil {
		log.Printf("Failed to add connection to epoll after upgrade: %v", err)
		// conn.Close() is called inside ep.add if EpollCtl fails.
		return
	}
	// Log successful upgrade and addition maybe? ep.add logs addition.
	// log.Printf("WebSocket connection established and added to epoll (FD obtained in add).")
}

// Starts the worker goroutine pool.
func startWorkers(n int, ep *epoll, jobChan <-chan eventJob, wg *sync.WaitGroup) {
	log.Printf("Starting %d worker goroutines...", n)
	for i := 0; i < n; i++ {
		wg.Add(1)
		// Pass worker ID for logging
		go workerFunc(i, ep, jobChan, wg)
	}
}

// Deletes connection from epoll, map, and closes the connection itself.
func (ep *epoll) deleteAndClose(conn net.Conn, fd int, reason string) {
	// Ensure we only try this once per connection if called concurrently
	// This relies on LoadAndDelete being atomic.

	log.Printf("Initiating deleteAndClose for FD %d. Reason: %s", fd, reason)

	// Attempt to remove from epoll and map
	// delete() now logs results internally
	_ = ep.delete(fd) // We ignore the error here as we will attempt Close anyway

	// Close the actual net.Conn
	log.Printf("Closing connection for FD %d...", fd)
	err := conn.Close()
	if err != nil {
		// It's common to get ErrClosed if it was already closed by another path or peer.
		if errors.Is(err, net.ErrClosed) {
			log.Printf("Connection for FD %d already closed (net.ErrClosed).", fd)
		} else {
			log.Printf("Error closing connection for FD %d: %v", fd, err)
		}
	} else {
		log.Printf("Successfully closed connection for FD %d.", fd)
	}
}

// Processes events for a specific file descriptor received from epoll.
func (ep *epoll) handleEvents(fd int, events uint32) {
	// Retrieve the connection from our map
	connVal, ok := ep.connections.Load(fd)
	if !ok {
		// Connection might have been closed and removed by another worker already.
		// Or it could be a spurious event for an already closed FD.
		// log.Printf("Ignoring events 0x%x for FD %d: Connection not found in map (likely already closed).", events, fd)
		// Attempt deletion just in case epoll still tracks it somehow (should fail with ENOENT).
		// _ = ep.delete(fd) // Be cautious, might hide other issues. Best to rely on initial closure path.
		return
	}
	conn := connVal.(net.Conn) // Assert type

	// Check for errors or hangup events first.
	// EPOLLRDHUP indicates the peer has closed their writing end or the whole connection.
	// EPOLLHUP often indicates an unexpected close or reset.
	// EPOLLERR indicates an error on the FD.
	if events&(unix.EPOLLERR|unix.EPOLLHUP|unix.EPOLLRDHUP) != 0 {
		errMsg := fmt.Sprintf("Error/Hangup event 0x%x", events)
		ep.deleteAndClose(conn, fd, errMsg)
		return
	}

	// Handle write readiness (EPOLLOUT)
	// Basic implementation: Assume ready to write if needed.
	// A real implementation would check if there's pending data to write and attempt writing.
	// If using EPOLLET, you'd need to write until it returns EAGAIN/EWOULDBLOCK.
	// if events&unix.EPOLLOUT != 0 {
	// log.Printf("FD %d ready for writing (EPOLLOUT)", fd)
	// Potentially call a handleWrite function if you have buffered writes
	// c.handleWrite()
	// Check if handleWrite closed the connection
	// if _, stillOk := ep.connections.Load(fd); !stillOk { return }
	// }

	// Handle read readiness (EPOLLIN)
	if events&unix.EPOLLIN != 0 {
		// log.Printf("FD %d ready for reading (EPOLLIN)", fd) // Can be noisy
		ep.handleRead(conn, fd)
		// Check if handleRead closed the connection
		// if _, stillOk := ep.connections.Load(fd); !stillOk { return }
	}
}

// Handles reading data from a connection that epoll indicated is ready.
func (ep *epoll) handleRead(conn net.Conn, fd int) {
	// Note: Using EPOLLET means we should read until we get an error,
	// typically EAGAIN or EWOULDBLOCK, indicating no more data for now.
	// wsutil.ReadClientData reads one full message, which might not be sufficient for EPOLLET.
	// For simplicity here, we read one message. A production EPOLLET server
	// might need a loop and non-blocking reads.

	// Read a single WebSocket message (frame).
	// Consider using wsutil.Reader for more complex scenarios (fragmentation, buffering).
	_, _, err := wsutil.ReadClientData(conn)

	if err != nil {
		// Determine appropriate reason string for logging closure
		var reason string
		if errors.Is(err, io.EOF) {
			reason = "EOF during read" // Often means clean closure by peer
		} else if errors.Is(err, net.ErrClosed) {
			reason = "Read on closed connection" // Connection already closed locally or remotely
		} else if errors.Is(err, syscall.EPIPE) {
			reason = "Broken pipe during read"
		} else if opErr, ok := err.(*net.OpError); ok {
			if errors.Is(opErr.Err, syscall.ECONNRESET) {
				reason = "Connection reset by peer during read"
			} else if errors.Is(opErr.Err, syscall.ETIMEDOUT) {
				reason = "Timeout during read"
			} else {
				reason = fmt.Sprintf("Network OpError during read: %v", err)
			}
		} else {
			reason = fmt.Sprintf("Unknown error during read: %T %v", err, err)
		}

		// If any error occurs during read, assume connection is defunct.
		log.Printf("Read error on FD %d: %v. Closing.", fd, err)
		ep.deleteAndClose(conn, fd, reason)
		return
	}

	// Successfully read a message. If needed, process the message 'msg'.
	// log.Printf("Received message on FD %d: %s", fd, string(msg)) // Example: Echo or process

	// IMPORTANT for EPOLLET: If wsutil.ReadClientData doesn't drain the kernel buffer,
	// you might not get another EPOLLIN event until more data arrives.
	// If this becomes an issue, you'd need non-blocking reads in a loop here.
}

// Function executed by worker goroutines.
func workerFunc(id int, ep *epoll, jobChan <-chan eventJob, wg *sync.WaitGroup) {
	defer wg.Done()
	log.Printf("Worker %d started.", id)
	for job := range jobChan {
		// log.Printf("Worker %d processing job for FD %d, Events: 0x%x", id, job.fd, job.events) // Verbose
		ep.handleEvents(job.fd, job.events)
	}
	log.Printf("Worker %d stopped.", id)
}

// Main application entry point.
func main() {
	flag.Parse()

	// --- Set higher file descriptor limits ---
	var rlimit syscall.Rlimit
	if err := syscall.Getrlimit(syscall.RLIMIT_NOFILE, &rlimit); err != nil {
		log.Printf("Warning: Error getting RLIMIT_NOFILE: %v", err)
	} else {
		log.Printf("Initial RLIMIT_NOFILE: cur=%d, max=%d", rlimit.Cur, rlimit.Max)
		// Set current limit to the maximum allowed hard limit
		rlimit.Cur = rlimit.Max
		if err := syscall.Setrlimit(syscall.RLIMIT_NOFILE, &rlimit); err != nil {
			log.Printf("Warning: Error setting RLIMIT_NOFILE to %d: %v", rlimit.Max, err)
		} else {
			// Verify the new limit
			if err := syscall.Getrlimit(syscall.RLIMIT_NOFILE, &rlimit); err == nil {
				log.Printf("Set RLIMIT_NOFILE: cur=%d, max=%d", rlimit.Cur, rlimit.Max)
			}
		}
	}
	// --- End FD limit setting ---

	// Buffered channel for jobs from epoll loop to workers
	jobChan := make(chan eventJob, *workers*2) // Buffer size can be tuned

	// Context for signalling shutdown to the epoll loop
	shutdownCtx, cancelEpollLoop := context.WithCancel(context.Background())
	defer cancelEpollLoop() // Ensure cancellation happens eventually

	// Create the epoll handler
	epoll, err := newEpoll(jobChan, shutdownCtx)
	if err != nil {
		// newEpoll now calls log.Fatalf on error, so this check is technically redundant
		log.Fatalf("Failed to initialize epoll: %v", err)
		// os.Exit(1) // Not reachable due to Fatalf
	}

	// Close the epoll file descriptor on shutdown
	defer func() {
		log.Printf("Closing epoll FD: %d", epoll.fd)
		if err := unix.Close(epoll.fd); err != nil {
			log.Printf("Error closing epoll FD %d: %v", epoll.fd, err)
		}
	}()

	// Start worker goroutines
	workerWg := &sync.WaitGroup{}
	startWorkers(*workers, epoll, jobChan, workerWg)

	// Start the epoll event loop in a separate goroutine
	go epoll.wait()

	// Configure HTTP server
	mux := http.NewServeMux()
	mux.HandleFunc("/ws", func(w http.ResponseWriter, r *http.Request) {
		wsHander(w, r, epoll)
	})

	srv := &http.Server{
		Addr:    *addr,
		Handler: mux,
		// Consider adding timeouts (ReadTimeout, WriteTimeout, IdleTimeout)
	}

	// Start HTTP server in a goroutine
	go func() {
		log.Printf("WebSocket server listening on %s", *addr)
		if err := srv.ListenAndServe(); err != nil && !errors.Is(err, http.ErrServerClosed) {
			log.Fatalf("HTTP server ListenAndServe error: %v", err)
		}
		log.Println("HTTP server stopped listening.")
	}()

	// --- Graceful Shutdown Handling ---
	quit := make(chan os.Signal, 1)
	signal.Notify(quit, syscall.SIGINT, syscall.SIGTERM)
	sig := <-quit
	log.Printf("Shutdown signal received: %s. Starting graceful shutdown...", sig.String())

	// 1. Stop accepting new HTTP connections
	shutdownCtxHTTP, cancelHTTP := context.WithTimeout(context.Background(), 15*time.Second) // Increased timeout
	defer cancelHTTP()
	if err := srv.Shutdown(shutdownCtxHTTP); err != nil {
		log.Printf("HTTP server shutdown error: %v", err)
	} else {
		log.Println("HTTP server finished accepting new connections.")
	}

	// 2. Signal epoll loop and workers to stop processing events
	log.Println("Signalling epoll loop and workers to stop...")
	cancelEpollLoop() // Signals epoll.wait() via shutdownCtx

	// 3. Close the job channel (signals workers to exit after finishing current job)
	//    Do this *after* cancelling the epoll loop to prevent new jobs being added.
	close(jobChan)
	log.Println("Worker job channel closed.")

	// 4. Wait for worker goroutines to finish their current tasks
	log.Println("Waiting for workers to finish...")
	workerWg.Wait()
	log.Println("All workers finished.")

	// 5. Close remaining connections (optional, workers should have handled most)
	//    This is a final cleanup; graceful closure should happen via epoll events/workers.
	log.Println("Performing final connection map cleanup...")
	closedCount := 0
	epoll.connections.Range(func(key, value interface{}) bool {
		fd := key.(int)
		conn := value.(net.Conn)
		log.Printf("Closing connection FD %d from final cleanup.", fd)
		conn.Close() // Ignore error here
		closedCount++
		return true // Continue iteration
	})
	log.Printf("Closed %d connections during final cleanup.", closedCount)
	log.Printf("Server gracefully shut down.")

	// Epoll FD is closed by defer earlier.
}
